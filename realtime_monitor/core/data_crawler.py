import asyncio
import logging
import os
import sys
import time
# import aiohttp
from datetime import datetime, timezone
from typing import List, Dict, Optional

import requests
from asgiref.sync import sync_to_async

import django
from django.utils import timezone as django_timezone

from realtime_monitor.utils.utils import _handle_conversations
from common.wechat_bot import send_wechat_message
from linkedin_realtime_monitor.settings import WechatRobotKey

# é¡¹ç›®æ ¹ç›®å½•è·¯å¾„ï¼ˆæ ¹æ®ä½ çš„å®é™…ç»“æ„è°ƒæ•´ï¼‰
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(BASE_DIR)

os.environ.setdefault("DJANGO_SETTINGS_MODULE", "linkedin_realtime_monitor.settings")  # æ›¿æ¢æˆä½ çš„ settings è·¯å¾„
django.setup()

from lkp_client_base_utils.lkp_client_base import LKPClientBase
from realtime_monitor.models import RealtimeConnection, RealtimeConversation, MonitorAccount
from realtime_monitor.core.db_health_check import db_health_checker


class DataCrawler:
    """æ•°æ®æŠ“å–å™¨"""

    def __init__(self, account_id: str):
        self.account_id = account_id

    async def crawl_connections(self, page, max_pages: Optional[int] = None) -> int:
        """æŠ“å–å¥½å‹åˆ—è¡¨ï¼ˆä½¿ç”¨ get_connections_v2 æ¥å£ï¼‰

        Args:
            page: Playwright page å¯¹è±¡
            max_pages: æœ€å¤§ç¿»é¡µæ¬¡æ•°ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰

        Returns:
            int: æ–°å¢å¥½å‹æ•°é‡
        """
        logging.info(f"Crawling connections for {self.account_id}, max_pages={max_pages}")

        # ç¡®ä¿æ•°æ®åº“è¿æ¥å¯ç”¨
        if not await db_health_checker.ensure_connection_async():
            logging.error(f"Database connection not available for crawl_connections")
            return 0

        # è·å–è´¦å·ä¿¡æ¯
        account = await MonitorAccount.objects.aget(id=int(self.account_id))
        sender_email = account.email

        # åˆå§‹åŒ– LKPClient
        lkpc = LKPClientBase('prod')

        # è·å–æ•°æ®åº“ä¸­æœ€æ–°çš„å¥½å‹ hash_idï¼ˆç”¨äºå»é‡ï¼‰
        latest_hash_id = await self._get_latest_connection_profile_id()

        start = 0
        count = 40
        raw_connections_data = []
        should_stop = False
        current_page = 0

        # å¾ªç¯è·å–æ‰€æœ‰æ–°å¥½å‹
        while not should_stop:
            try:

                # è°ƒç”¨ LinkedIn get_connections_v2 API
                lk_connections_data = lkpc.make_a_linked_in_request(
                    sender_email,
                    category='extended',
                    method_name='get_connections_v2',
                    params={"start": start, 'count': count}
                )

                if not lk_connections_data:
                    logging.warning(f"API è¿”å›ç©ºæ•°æ®: start={start}, count={count}")
                    break

                connections_data = lk_connections_data.get('elements', [])

                if not connections_data:
                    logging.info(f"æ²¡æœ‰æ›´å¤šè¿æ¥æ•°æ®ï¼Œåœæ­¢è¯·æ±‚")
                    break

                # æ£€æŸ¥å»é‡ï¼šå¦‚æœé‡åˆ°å·²å­˜åœ¨çš„å¥½å‹ï¼Œåœæ­¢
                for conn_info in connections_data:
                    profile_dict = conn_info.get('connectedMemberResolutionResult', {})
                    if profile_dict:
                        entity_urn = profile_dict.get('entityUrn', '')
                        hash_id = entity_urn.split(':')[-1] if entity_urn else None

                        # é‡åˆ°å·²å­˜åœ¨çš„å¥½å‹ï¼Œåœæ­¢
                        if hash_id == latest_hash_id:
                            should_stop = True
                            break

                        raw_connections_data.append(conn_info)

                if should_stop:
                    break

                # å¦‚æœè¿”å›çš„æ•°æ®å°‘äºè¯·æ±‚çš„æ•°é‡ï¼Œè¯´æ˜æ²¡æœ‰æ›´å¤šæ•°æ®äº†
                if len(connections_data) < count:
                    logging.info(f"è¿”å›æ•°æ®å°‘äºè¯·æ±‚æ•°é‡ ({len(connections_data)} < {count})ï¼Œåœæ­¢è¯·æ±‚")
                    break

                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§ç¿»é¡µæ¬¡æ•°é™åˆ¶
                current_page += 1
                if max_pages is not None and current_page >= max_pages:
                    logging.info(f"è¾¾åˆ°æœ€å¤§ç¿»é¡µæ¬¡æ•°é™åˆ¶ ({current_page} >= {max_pages})ï¼Œåœæ­¢è¯·æ±‚")
                    break

                start += count
                await asyncio.sleep(2)  # åˆ†é¡µé—´éš”

            except Exception as e:
                logging.error(f"è·å–è¿æ¥æ•°æ®å¤±è´¥: {e}", exc_info=True)
                break

        if raw_connections_data:
            # æ‰¹é‡æŸ¥è¯¢ member_idï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰

            # è§£æ connections æ•°æ®
            parsed_connections = []

            for connection_info in raw_connections_data:
                try:
                    conn_data = self._parse_connection_data(connection_info)
                    if conn_data:
                        parsed_connections.append(conn_data)
                except Exception as e:
                    logging.warning(f"è§£æè¿æ¥æ•°æ®å¤±è´¥: {str(e)}")
                    continue

            # ä¿å­˜åˆ°æ•°æ®åº“
            if parsed_connections:
                saved_connections = await self._save_connections_v2(parsed_connections)
                logging.info(f"Saved {len(saved_connections)} new connections")

                # é€šçŸ¥ Business æ–¹
                notification_success = await self._notify_business_conversations(saved_connections, 'my_network')

                # å¦‚æœé€šçŸ¥æˆåŠŸï¼Œæ¸…é™¤ My Network çº¢ç‚¹
                if notification_success:
                    await self._clear_notification(page, notification_source='my_network')
            else:
                logging.info(f"No new connections found after parsing")
        else:
            logging.info(f"No new connections data retrieved from API")
            await self._clear_notification(page, notification_source='my_network')
            parsed_connections = []
        return len(parsed_connections)

    async def crawl_conversations(self, page) -> int:
        """æŠ“å–å¯¹è¯åˆ—è¡¨

        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. å…ˆè·å–DBä¸­å½“å‰è´¦å·çš„æœ€å¤§æ¶ˆæ¯æ—¶é—´ä½œä¸ºåŸºå‡†
        2. è¯·æ±‚ç¬¬ä¸€é¡µåï¼Œæ£€æŸ¥æœ€åä¸€æ¡çš„æ—¶é—´
        3. åªå¤„ç†å’Œä¿å­˜æ—¶é—´ > DBåŸºå‡†æ—¶é—´çš„å¯¹è¯
        4. å¦‚æœæœ€åä¸€æ¡æ—¶é—´ â‰¤ DBåŸºå‡†æ—¶é—´ï¼Œåœæ­¢ç¿»é¡µ

        ä½¿ç”¨ conversations_by_sync_tokenï¼ˆç¬¬ä¸€é¡µï¼‰å’Œ conversations_by_categoryï¼ˆç¿»é¡µï¼‰æ¥å£
        LinkedIn æŒ‰ last_message_delivered_at ä»å¤§åˆ°å°ï¼ˆé™åºï¼‰è¿”å›å¯¹è¯

        Returns:
            int: æ›´æ–°çš„å¯¹è¯æ•°é‡
        """
        logging.info(f"Crawling conversations for {self.account_id}")

        # ç¡®ä¿æ•°æ®åº“è¿æ¥å¯ç”¨
        if not await db_health_checker.ensure_connection_async():
            logging.error(f"Database connection not available for crawl_conversations")
            return 0

        # è·å–è´¦å·ä¿¡æ¯ï¼Œç”¨äºè°ƒç”¨ LKP æ¥å£
        account = await MonitorAccount.objects.aget(id=int(self.account_id))
        sender_email = account.email
        hash_id = account.hash_id

        lkpc = LKPClientBase('prod')

        # âš¡ å…³é”®ä¼˜åŒ–ï¼šå…ˆè·å–DBä¸­å½“å‰è´¦å·çš„æœ€å¤§æ¶ˆæ¯æ—¶é—´
        db_max_time = await self._get_max_message_time()
        logging.info(f"DB max message time: {db_max_time}")

        if not hash_id:
            try:
                connection_response = lkpc.make_a_linked_in_request(sender_email, 'extended', 'connection_summary', {})
                entity_urn = connection_response.get('entityUrn', "") if connection_response else ""
                hash_id = entity_urn.split(":")[-1] if entity_urn else ""
                if hash_id:
                    account.hash_id = hash_id
                    await sync_to_async(account.save)()
            except Exception as e:
                logging.error(f"Failed to fetch hash_id for account {self.account_id}: {e}", exc_info=True)
                # å¦‚æœè·å– hash_id å¤±è´¥ï¼Œç»§ç»­å°è¯•ä½¿ç”¨ç©ºçš„ hash_idï¼ˆå¯èƒ½ä¼šåœ¨åç»­å¤±è´¥ï¼‰

        if not hash_id:
            logging.warning(f"Account {self.account_id} has no hash_id, skipping conversation crawl")
            return 0

        all_messages = []

        # å¾ªç¯è¯·æ±‚å¤šé¡µæ•°æ®, æ¯é¡µ20æ¡ï¼Œ æœ€å¤š10é¡µ
        for page_num in range(10):
            try:
                if page_num == 0:
                    # ç¬¬ä¸€é¡µï¼šä½¿ç”¨ conversations_by_sync_token
                    response = lkpc.make_a_linked_in_request(
                        sender_email,
                        category='extended',
                        method_name='conversations_by_sync_token',
                        params={'fsd_profile': hash_id}
                    )
                else:
                    # ç¿»é¡µï¼šä½¿ç”¨ conversations_by_category
                    # ä»ä¸Šä¸€é¡µæœ€åä¸€æ¡æ¶ˆæ¯è·å– last_activity_at
                    if not all_messages:
                        break  # å¦‚æœä¸Šä¸€é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ

                    last_message = all_messages[-1]
                    last_activity_at_iso = last_message.get('last_activity_at')

                    if not last_activity_at_iso:
                        break  # å¦‚æœæ²¡æœ‰ last_activity_atï¼Œæ— æ³•ç¿»é¡µ

                    # å°† ISO æ ¼å¼å­—ç¬¦ä¸²è½¬æ¢å›æ—¶é—´æˆ³ï¼ˆæ¯«ç§’çº§ï¼‰ç”¨äº API è¯·æ±‚
                    try:
                        # è§£æ ISO æ ¼å¼å­—ç¬¦ä¸²ä¸º datetime å¯¹è±¡
                        dt = datetime.fromisoformat(last_activity_at_iso.replace('Z', '+00:00'))
                        # ç¡®ä¿æœ‰æ—¶åŒºä¿¡æ¯
                        if dt.tzinfo is None:
                            dt = dt.replace(tzinfo=timezone.utc)

                        # å¦‚æœæ•°æ®åº“ä¸­çš„æœ€å¤§æ—¶é—´ä¸ä¸ºç©ºï¼Œä¸”å½“å‰æ—¶é—´å°äºç­‰äºæœ€å¤§æ—¶é—´ï¼Œåœæ­¢ç¿»é¡µ
                        # ç¡®ä¿ db_max_time ä¹Ÿæœ‰æ—¶åŒºä¿¡æ¯è¿›è¡Œæ¯”è¾ƒ
                        if db_max_time:
                            # å¦‚æœ db_max_time æ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œæ·»åŠ  UTC æ—¶åŒº
                            if db_max_time.tzinfo is None:
                                db_max_time_utc = db_max_time.replace(tzinfo=timezone.utc)
                            else:
                                db_max_time_utc = db_max_time
                            
                            if dt <= db_max_time_utc:
                                logging.info(
                                    f"ğŸ›‘ Last conversation time ({dt}) â‰¤ DB max time ({db_max_time_utc}), "
                                    f"stopping pagination at page {page_num}"
                                )
                                break

                        # è½¬æ¢ä¸ºæ¯«ç§’çº§æ—¶é—´æˆ³
                        last_activity_at_timestamp = int(dt.timestamp() * 1000)
                    except (ValueError, AttributeError) as e:
                        logging.warning(f"Failed to parse last_activity_at for pagination: {e}")
                        break  # å¦‚æœè§£æå¤±è´¥ï¼Œåœæ­¢ç¿»é¡µ

                    response = lkpc.make_a_linked_in_request(
                        sender_email,
                        category='extended',
                        method_name='conversations_by_category',
                        params={
                            'fsd_profile': hash_id,
                            'last_activity_at': last_activity_at_timestamp
                        }
                    )
            except Exception as e:
                logging.error(f"Failed to fetch conversations page {page_num}: {e}", exc_info=True)
                break  # API è°ƒç”¨å¤±è´¥ï¼Œåœæ­¢ç¿»é¡µ

            # è§£æå“åº”æ•°æ®
            if not response:
                break  # å¦‚æœå“åº”ä¸ºç©ºï¼Œåœæ­¢ç¿»é¡µ

            # æå–å¯¹è¯åˆ—è¡¨
            # å¤„ç†ä¸åŒçš„å“åº”ç»“æ„ï¼šå¯èƒ½æ˜¯ {'json': {...}} æˆ– {'data': {...}}
            conversation_data = response.get('json', {}) or response.get('data', {})
            data_section = conversation_data.get('data', conversation_data)

            # å°è¯•ä»ä¸åŒçš„å­—æ®µè·å– elements
            if page_num == 0:
                # ç¬¬ä¸€é¡µä½¿ç”¨ messengerConversationsBySyncToken
                data_node = data_section.get('messengerConversationsBySyncToken', {})
                elements = data_node.get('elements', [])
            else:
                # ç¿»é¡µä½¿ç”¨ conversations_by_category
                # ç¿»é¡µåçš„å“åº”ç»“æ„å¯èƒ½ä¸åŒï¼Œå°è¯•å¤šç§å¯èƒ½çš„è·¯å¾„
                data_node = data_section.get('messengerConversationsByCategory', {}) or \
                            data_section.get('messengerConversationsBySyncToken', {})
                elements = data_node.get('elements', [])

                # å¦‚æœ elements ä¸ºç©ºï¼Œå¯èƒ½æ˜¯ç›´æ¥è¿”å›å¯¹è¯é¡¹åˆ—è¡¨
                if not elements and isinstance(data_section, list):
                    elements = data_section
                elif not elements and isinstance(data_node, list):
                    elements = data_node

            if not elements:
                break  # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ

            # ä½¿ç”¨ sync_to_async åŒ…è£…åŒæ­¥å‡½æ•°è°ƒç”¨ï¼Œé¿å…åœ¨å¼‚æ­¥ä¸Šä¸‹æ–‡ä¸­ç›´æ¥è°ƒç”¨åŒæ­¥æ•°æ®åº“æ“ä½œ
            current_messages = await sync_to_async(_handle_conversations)(elements, hash_id)
            all_messages += current_messages

        logging.info('all_messages: {}'.format(len(all_messages)))
        # å¤„ç† all_messagesï¼Œè·å–æœ€æ–°çš„å¯¹è¯æ¶ˆæ¯ï¼Œåªæ–°å¢æˆ–æ›´æ–° last_activity_at > db_max_time çš„å¯¹è¯
        # ä¿å­˜åˆ°æ•°æ®åº“
        updated_count, updated_convs = await self._save_conversations_from_all_messages(
            all_messages,
            db_max_time
        )
        total_updated = updated_count
        all_updated_conversations = updated_convs

        # é€šçŸ¥ Business æ–¹
        if all_updated_conversations:
            notification_success = await self._notify_business_conversations(all_updated_conversations, 'message')

            # å¦‚æœé€šçŸ¥æˆåŠŸï¼Œæ¸…é™¤ message çº¢ç‚¹
            if notification_success:
                await self._clear_notification(page, notification_source='message')
        else:
            logging.info(f"No new or updated conversations to notify")
            await self._clear_notification(page, notification_source='message')

        logging.info(f"Processed {total_updated} conversations for {self.account_id}")
        return total_updated

    async def _save_conversations_from_all_messages(
            self,
            all_messages: List[dict],
            db_max_time: Optional[datetime]
    ) -> tuple[int, List[dict]]:
        """ä» all_messages ä¸­ä¿å­˜å¯¹è¯æ•°æ®åˆ°æ•°æ®åº“

        æ ¹æ®æ³¨é‡Šä¸­çš„æ•°æ®ç»“æ„ï¼Œå¤„ç† _handle_conversations è¿”å›çš„æ ¼å¼åŒ–æ•°æ®

        Args:
            all_messages: _handle_conversations è¿”å›çš„æ ¼å¼åŒ–å¯¹è¯åˆ—è¡¨
            db_max_time: æ•°æ®åº“ä¸­å½“å‰è´¦å·çš„æœ€å¤§ last_activity_at æ—¶é—´ï¼ˆåŸºå‡†æ—¶é—´ï¼‰

        Returns:
            tuple[int, List[dict]]: (æ›´æ–°çš„å¯¹è¯æ•°é‡, æ›´æ–°çš„å¯¹è¯æ•°æ®åˆ—è¡¨)
        """
        # ç¡®ä¿æ•°æ®åº“è¿æ¥å¯ç”¨
        if not await db_health_checker.ensure_connection_async():
            logging.error(f"Database connection not available for saving conversations")
            return 0, []
        
        updated_count = 0
        updated_conversations: List[dict] = []
        account = await MonitorAccount.objects.aget(id=int(self.account_id))

        for msg in all_messages:
            try:

                hash_id = msg.get('hash_id', '')

                # è§£ææ—¶é—´å­—æ®µï¼ˆISO æ ¼å¼å­—ç¬¦ä¸² -> datetimeï¼‰
                last_activity_at_str = msg.get('last_activity_at')
                if not last_activity_at_str:
                    logging.warning(f"Conversation {hash_id} missing last_activity_at, skipping")
                    continue

                # å°† ISO æ ¼å¼å­—ç¬¦ä¸²è½¬æ¢ä¸º datetime å¯¹è±¡
                try:
                    last_activity_at = datetime.fromisoformat(
                        last_activity_at_str.replace('Z', '+00:00')
                    )
                    # ç¡®ä¿æœ‰æ—¶åŒºä¿¡æ¯
                    if last_activity_at.tzinfo is None:
                        last_activity_at = last_activity_at.replace(tzinfo=timezone.utc)
                except (ValueError, AttributeError) as e:
                    logging.warning(f"Failed to parse last_activity_at for {hash_id}: {e}")
                    continue

                # å…³é”®è¿‡æ»¤ï¼šåªå¤„ç†æ—¶é—´ > db_max_time çš„å¯¹è¯
                if db_max_time and last_activity_at <= db_max_time:
                    logging.debug(
                        f"â­ï¸ Skipping old conversation: {hash_id} "
                        f"(API: {last_activity_at} â‰¤ DB: {db_max_time})"
                    )
                    continue  # è·³è¿‡æ—§å¯¹è¯

                # è§£æå…¶ä»–æ—¶é—´å­—æ®µ
                created_at = None
                if msg.get('created_at'):
                    try:
                        created_at = datetime.fromisoformat(
                            msg['created_at'].replace('Z', '+00:00')
                        )
                        if created_at.tzinfo is None:
                            created_at = created_at.replace(tzinfo=timezone.utc)
                    except (ValueError, AttributeError):
                        pass

                last_read_at = None
                if msg.get('last_read_at'):
                    try:
                        last_read_at = datetime.fromisoformat(
                            msg['last_read_at'].replace('Z', '+00:00')
                        )
                        if last_read_at.tzinfo is None:
                            last_read_at = last_read_at.replace(tzinfo=timezone.utc)
                    except (ValueError, AttributeError):
                        pass

                # è§£ææœ€åä¸€æ¡æ¶ˆæ¯çš„æ—¶é—´
                last_message = msg.get('last_message', {})
                last_message_delivered_at = None
                if last_message.get('delivered_at'):
                    try:
                        last_message_delivered_at = datetime.fromisoformat(
                            last_message['delivered_at'].replace('Z', '+00:00')
                        )
                        if last_message_delivered_at.tzinfo is None:
                            last_message_delivered_at = last_message_delivered_at.replace(tzinfo=timezone.utc)
                    except (ValueError, AttributeError):
                        pass
                # æŸ¥è¯¢æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨è¯¥å¯¹è¯ï¼ˆåŸºäº account å’Œ hash_id ä¸€èµ·åšå”¯ä¸€æ€§åˆ¤æ–­ï¼‰
                existing_conv = await RealtimeConversation.objects.filter(
                    account=account,
                    hash_id=hash_id
                ).afirst()

                # å‡†å¤‡å¯¹è¯æ•°æ®
                conv_data = {
                    'hash_id': hash_id,
                    'conversation_id': msg.get('conversation_id', ''),
                    'public_id': msg.get('public_id', ''),
                    'member_id': msg.get('member_id', ''),
                    'conversation_url': msg.get('conversation_url', ''),
                    'first_name': msg.get('first_name', ''),
                    'last_name': msg.get('last_name', ''),
                    'distance': msg.get('distance', ''),
                    'unread_count': msg.get('unread_count', 0),
                    'dialogue_created_at': created_at,
                    'last_activity_at': last_activity_at,
                    'last_read_at': last_read_at,
                    'is_group_chat': msg.get('is_group_chat', False),
                    'last_message_text': last_message.get('text', ''),
                    'last_message_sender': last_message.get('sender', ''),
                    'last_message_delivered_at': last_message_delivered_at,
                    'source': msg.get('source', 'original'),
                }

                if existing_conv is None:
                    # ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°å¯¹è¯
                    # æ³¨æ„ï¼šhash_id å·²ç»åœ¨ conv_data ä¸­ï¼Œä¸éœ€è¦å•ç‹¬ä¼ é€’
                    await RealtimeConversation.objects.acreate(
                        account=account,
                        **conv_data
                    )
                    logging.info(f"âœ… Created new conversation: {hash_id}")
                    updated_count += 1
                    updated_conversations.append(conv_data)
                else:
                    # å·²å­˜åœ¨ï¼Œæ›´æ–°å¯¹è¯ä¿¡æ¯
                    existing_conv.conversation_id = conv_data['conversation_id']
                    existing_conv.public_id = conv_data['public_id']
                    existing_conv.member_id = conv_data['member_id']
                    existing_conv.conversation_url = conv_data['conversation_url']
                    existing_conv.first_name = conv_data['first_name']
                    existing_conv.last_name = conv_data['last_name']
                    existing_conv.distance = conv_data['distance']
                    existing_conv.unread_count = conv_data['unread_count']
                    existing_conv.dialogue_created_at = conv_data['dialogue_created_at']
                    existing_conv.last_activity_at = conv_data['last_activity_at']
                    existing_conv.last_read_at = conv_data['last_read_at']
                    existing_conv.is_group_chat = conv_data['is_group_chat']
                    existing_conv.last_message_text = conv_data['last_message_text']
                    existing_conv.last_message_sender = conv_data['last_message_sender']
                    existing_conv.last_message_delivered_at = conv_data['last_message_delivered_at']
                    existing_conv.source = conv_data['source']

                    # ä½¿ç”¨ sync_to_async åŒ…è£…åŒæ­¥çš„ save() æ–¹æ³•
                    await sync_to_async(existing_conv.save)()
                    logging.info(
                        f"âœ… Updated conversation: {hash_id} "
                        f"(last_activity_at: {last_activity_at})"
                    )
                    updated_count += 1
                    updated_conversations.append(conv_data)

            except Exception as e:
                logging.error(f"Error processing conversation: {e}", exc_info=True)
                continue

        return updated_count, updated_conversations

    async def _get_max_message_time(self) -> Optional[datetime]:
        """è·å–å½“å‰è´¦å·åœ¨æ•°æ®åº“ä¸­çš„æœ€å¤§æ¶ˆæ¯æ—¶é—´

        ç”¨ä½œåŸºå‡†æ—¶é—´ï¼Œåªå¤„ç†æ¯”è¿™ä¸ªæ—¶é—´æ›´æ–°çš„å¯¹è¯

        Returns:
            Optional[datetime]: æœ€å¤§æ¶ˆæ¯æ—¶é—´ï¼Œå¦‚æœæ²¡æœ‰è®°å½•åˆ™è¿”å› None
        """
        from django.db.models import Max

        # æ³¨æ„ï¼šRealtimeConversation ä¸­çš„å¤–é”®å­—æ®µåä¸º accountï¼ˆdb_column='account_id'ï¼‰ï¼Œ
        # æŸ¥è¯¢æ—¶åº”ä½¿ç”¨ account_id æˆ– account__idï¼Œè€Œä¸æ˜¯ account__account_id
        try:
            account_id = int(self.account_id)
        except (TypeError, ValueError):
            account_id = self.account_id

        result = await RealtimeConversation.objects.filter(
            account_id=account_id
        ).aaggregate(max_time=Max('last_activity_at'))

        max_time = result.get('max_time')
        return max_time

    async def _fetch_api(self, page, url: str, max_retries: int = 3) -> dict:
        """è°ƒç”¨ LinkedIn APIï¼ˆå¸¦é‡è¯•ï¼‰"""
        for attempt in range(max_retries):
            try:
                response = await page.evaluate(
                    f"""
                    async () => {{
                        const response = await fetch('{url}', {{
                            credentials: 'include'
                        }});
                        return await response.json();
                    }}
                    """
                )
                return response

            except Exception as e:
                wait_time = (2 ** attempt) * 1
                logging.error(f"API fetch error (attempt {attempt + 1}): {e}")

                if attempt < max_retries - 1:
                    await asyncio.sleep(wait_time)
                    return {}
                else:
                    raise
        return {}

    async def _get_latest_connection_profile_id(self) -> Optional[str]:
        """è·å–æœ€æ–°çš„å¥½å‹ Profile IDï¼ˆåŸºäº hash_id å»é‡ï¼‰"""
        # ç¡®ä¿æ•°æ®åº“è¿æ¥å¯ç”¨
        if not await db_health_checker.ensure_connection_async():
            logging.warning(f"Database connection not available for getting latest connection")
            return None
        
        account = await MonitorAccount.objects.aget(id=int(self.account_id))
        latest = await RealtimeConnection.objects.filter(
            account=account
        ).order_by('-connected_at').afirst()

        return latest.hash_id if latest else None

    async def _save_connections(self, connections: List[dict]) -> List[dict]:
        """æ‰¹é‡ä¿å­˜å¥½å‹æ•°æ®

        Returns:
            List[dict]: ä¿å­˜çš„å¥½å‹æ•°æ®åˆ—è¡¨
        """
        objects = []
        saved_data = []

        for conn in connections:
            conn_data = {
                'account_id': self.account_id,
                'profile_id': self._extract_profile_id(conn),
                'profile_urn': conn.get('entityUrn'),
                'full_name': self._extract_name(conn),
                'headline': self._extract_headline(conn),
                'connected_at': self._parse_timestamp(conn.get('createdAt'))
            }

            objects.append(RealtimeConnection(**conn_data))
            saved_data.append(conn_data)  # æ”¶é›†ä¿å­˜çš„æ•°æ®

        await RealtimeConnection.objects.abulk_create(
            objects,
            ignore_conflicts=True
        )

        return saved_data

    @staticmethod
    def _extract_profile_id(conn: dict) -> str:
        """æå– Profile ID"""
        return conn.get('entityUrn', '').split(':')[-1]

    @staticmethod
    def _extract_name(conn: dict) -> str:
        """æå–å§“å"""
        # æ ¹æ®å®é™… API å“åº”ç»“æ„æå–
        return conn.get('connectedMember', {}).get('firstName', '') + ' ' + \
            conn.get('connectedMember', {}).get('lastName', '')

    @staticmethod
    def _extract_headline(conn: dict) -> str:
        """æå–æ ‡é¢˜"""
        return conn.get('connectedMember', {}).get('headline', '')

    @staticmethod
    def _parse_timestamp(ts) -> Optional[datetime]:
        """è§£ææ—¶é—´æˆ³"""
        if not ts:
            return None
        return datetime.fromtimestamp(ts / 1000)  # LinkedIn ä½¿ç”¨æ¯«ç§’æ—¶é—´æˆ³

    @staticmethod
    def _normalize_timestamp_to_utc(timestamp: Optional[int]) -> Optional[int]:
        """
        å°†æ—¶é—´æˆ³è§„èŒƒåŒ–ä¸º UTC+0 æ—¶é—´æˆ³ï¼ˆæ¯«ç§’çº§ï¼‰

        Args:
            timestamp: æ—¶é—´æˆ³ï¼ˆå¯èƒ½æ˜¯ç§’çº§æˆ–æ¯«ç§’çº§ï¼Œå¿…é¡»æ˜¯ UTC æ—¶é—´æˆ³ï¼‰

        Returns:
            UTC+0 çš„æ¯«ç§’çº§æ—¶é—´æˆ³ï¼Œå¦‚æœè¾“å…¥æ— æ•ˆè¿”å› None
        """
        if timestamp is None or timestamp == 0:
            return None

        try:
            # åˆ¤æ–­æ˜¯æ¯«ç§’çº§è¿˜æ˜¯ç§’çº§æ—¶é—´æˆ³
            if timestamp > 1e10:
                # æ¯«ç§’çº§æ—¶é—´æˆ³ï¼Œç›´æ¥è¿”å›
                return int(timestamp)
            else:
                # ç§’çº§æ—¶é—´æˆ³ï¼Œè½¬æ¢ä¸ºæ¯«ç§’çº§
                return int(timestamp * 1000)
        except (ValueError, TypeError, OverflowError):
            return None

    @staticmethod
    def _timestamp_to_iso_utc(timestamp: Optional[int]) -> Optional[str]:
        """
        å°†æ—¶é—´æˆ³è½¬æ¢ä¸º ISO 8601 æ ¼å¼çš„ UTC+0 æ—¶é—´å­—ç¬¦ä¸²

        Args:
            timestamp: æ—¶é—´æˆ³ï¼ˆå¯èƒ½æ˜¯ç§’çº§æˆ–æ¯«ç§’çº§ï¼Œå¿…é¡»æ˜¯ UTC æ—¶é—´æˆ³ï¼‰

        Returns:
            ISO 8601 æ ¼å¼çš„æ—¶é—´å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ï¼š'2025-09-24T06:44:22+00:00'
            å¦‚æœè¾“å…¥æ— æ•ˆè¿”å› None
        """
        if timestamp is None or timestamp == 0:
            return None

        try:
            # è§„èŒƒåŒ–æ—¶é—´æˆ³ä¸ºæ¯«ç§’çº§
            normalized = DataCrawler._normalize_timestamp_to_utc(timestamp)
            if normalized is None:
                return None

            # è½¬æ¢ä¸ºç§’çº§æ—¶é—´æˆ³
            time_int = normalized / 1000

            # è½¬æ¢ä¸º UTC æ—¶é—´çš„ datetime å¯¹è±¡
            dt = datetime.fromtimestamp(time_int, tz=timezone.utc)

            # æ ¼å¼åŒ–ä¸º ISO 8601 æ ¼å¼ï¼Œç¡®ä¿ä½¿ç”¨ +00:00 è€Œä¸æ˜¯ Z
            iso_str = dt.isoformat()
            # å°† Z æ›¿æ¢ä¸º +00:00 ä»¥ç¡®ä¿æ ¼å¼ç»Ÿä¸€
            if iso_str.endswith('Z'):
                iso_str = iso_str[:-1] + '+00:00'
            # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œæ·»åŠ  +00:00
            elif '+' not in iso_str and '-' not in iso_str[-6:]:
                iso_str = iso_str + '+00:00'

            return iso_str
        except (ValueError, TypeError, OverflowError, OSError):
            return None

    def _parse_connection_data(self, connection_info: Dict) -> Optional[Dict]:
        """
        è§£æå•ä¸ªè¿æ¥æ•°æ®ï¼ˆä¸ linkedin_interaction.py ä¿æŒä¸€è‡´ï¼‰

        Args:
            connection_info: LinkedIn API è¿”å›çš„è¿æ¥ä¿¡æ¯

        Returns:
            æ ¼å¼åŒ–çš„è¿æ¥æ•°æ®å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› None
        """
        profile_dict = connection_info.get('connectedMemberResolutionResult', {})
        if not profile_dict:
            return None

        # æå–åŸºæœ¬ä¿¡æ¯
        public_id = profile_dict.get('publicIdentifier')
        first_name = profile_dict.get('firstName')
        last_name = profile_dict.get('lastName')
        headline = profile_dict.get('headline')

        # æå– hash_id
        entity_urn = profile_dict.get('entityUrn', '')
        hash_id = entity_urn.split(':')[-1] if entity_urn else None

        # æå–è¿æ¥æ—¶é—´
        created_at = connection_info.get('createdAt')
        connected_at = None
        if created_at:
            if isinstance(created_at, int):
                # å°†æ—¶é—´æˆ³è½¬æ¢ä¸º ISO 8601 æ ¼å¼çš„ UTC+0 æ—¶é—´å­—ç¬¦ä¸²
                connected_at = self._timestamp_to_iso_utc(created_at)
            elif isinstance(created_at, str):
                # å¦‚æœå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•è§£æå¹¶ç¡®ä¿æ˜¯ UTC+0 æ ¼å¼
                try:
                    dt = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    if dt.tzinfo is None:
                        # å¦‚æœæ²¡æœ‰æ—¶åŒºä¿¡æ¯ï¼Œå‡è®¾æ˜¯ UTC
                        dt = dt.replace(tzinfo=timezone.utc)
                    connected_at = dt.isoformat()
                except (ValueError, AttributeError):
                    connected_at = created_at

        # æ„å»ºè¿”å›æ•°æ®
        return {
            'first_name': first_name,
            'last_name': last_name,
            'headline': headline,
            'public_id': public_id,
            'hash_id': hash_id,
            'connected_at': connected_at,
        }

    async def _save_connections_v2(self, connections: List[dict]) -> List[dict]:
        """æ‰¹é‡ä¿å­˜å¥½å‹æ•°æ®ï¼ˆä½¿ç”¨æ–°çš„æ•°æ®ç»“æ„ï¼‰

        Returns:
            List[dict]: ä¿å­˜çš„å¥½å‹æ•°æ®åˆ—è¡¨
        """
        # ç¡®ä¿æ•°æ®åº“è¿æ¥å¯ç”¨
        if not await db_health_checker.ensure_connection_async():
            logging.error(f"Database connection not available for saving connections")
            return []
        
        # è·å– account å¯¹è±¡
        account = await MonitorAccount.objects.aget(id=int(self.account_id))

        objects = []
        saved_data = []

        for conn_data in connections:
            # è½¬æ¢ connected_at å­—ç¬¦ä¸²ä¸º datetime å¯¹è±¡
            connected_at = None
            if conn_data.get('connected_at'):
                try:
                    connected_at_str = conn_data['connected_at']
                    if isinstance(connected_at_str, str):
                        # è§£æ ISO 8601 æ ¼å¼çš„å­—ç¬¦ä¸²
                        connected_at = datetime.fromisoformat(
                            connected_at_str.replace('Z', '+00:00'))
                    elif isinstance(connected_at_str, datetime):
                        connected_at = connected_at_str
                except (ValueError, AttributeError):
                    logging.warning(
                        f"Failed to parse connected_at: {conn_data.get('connected_at')}")

            conn_obj = RealtimeConnection(
                account=account,
                first_name=conn_data.get('first_name'),
                last_name=conn_data.get('last_name'),
                headline=conn_data.get('headline'),
                public_id=conn_data.get('public_id'),
                hash_id=conn_data.get('hash_id'),
                member_id=conn_data.get('member_id'),
                source=conn_data.get('source', 'original'),
                connected_at=connected_at or django_timezone.now()
            )
            objects.append(conn_obj)
            saved_data.append(conn_data)

        # ä½¿ç”¨ bulk_create æ‰¹é‡ä¿å­˜ï¼Œå¿½ç•¥å†²çªï¼ˆåŸºäº unique_together: account + member_idï¼‰
        await sync_to_async(RealtimeConnection.objects.bulk_create)(
            objects,
            ignore_conflicts=True
        )

        return saved_data

    async def _clear_notification(self, page, notification_source: str):
        """æ¸…é™¤çº¢ç‚¹ï¼ˆé€šè¿‡å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢ï¼‰

        Args:
            page: Playwright page å¯¹è±¡
            notification_source: 'my_network' æˆ– 'message'ï¼Œå†³å®šè·³è½¬åˆ°å“ªä¸ªé¡µé¢
        """
        try:
            logging.info(f'å¼€å§‹æ¸…é™¤çº¢ç‚¹: {notification_source}')
            
            # ç¡®å®šç›®æ ‡ URLï¼ˆç›´æ¥å¯¼èˆªï¼Œé¿å…é‡å®šå‘é—®é¢˜ï¼‰
            if notification_source == 'my_network':
                target_url = 'https://www.linkedin.com/mynetwork/grow/'
                wait_selector = 'button[aria-label*="Connect"]'  # Grow é¡µé¢çš„ç‰¹å¾å…ƒç´ 
                wait_description = "Connect button on Grow page"
            else:
                target_url = 'https://www.linkedin.com/messaging/'
                wait_selector = 'div[class*="msg-conversations-container"]'
                wait_description = "Messaging conversations container"
            
            # ç›´æ¥å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            try:
                logging.info(f"Navigating to {target_url}")
                await page.goto(target_url, timeout=60000)
                await page.wait_for_load_state('domcontentloaded', timeout=10000)
                logging.info(f"âœ… Navigated to {target_url}")
            except Exception as nav_err:
                logging.error(f"âŒ Failed to navigate to {target_url}: {nav_err}")
                pass
            
            # ç­‰å¾…é¡µé¢ç‰¹å¾å…ƒç´ åŠ è½½ï¼ˆç¡®è®¤é¡µé¢åŠ è½½æˆåŠŸï¼‰
            try:
                await page.wait_for_selector(wait_selector, timeout=10000)
                logging.info(f"âœ… Page loaded successfully (found {wait_description})")
            except Exception as wait_err:
                # å³ä½¿æ‰¾ä¸åˆ°ç‰¹å¾å…ƒç´ ï¼Œä¹Ÿç»§ç»­ï¼ˆé¡µé¢å¯èƒ½å·²åŠ è½½ï¼Œåªæ˜¯å…ƒç´ ç»“æ„å˜äº†ï¼‰
                logging.warning(
                    f"âš ï¸ Timeout waiting for {wait_description}: {wait_err}. "
                    f"Page might still be loaded, continuing..."
                )
            
            # ç­‰å¾…ä¸€ä¸‹ï¼Œç¡®ä¿çº¢ç‚¹è¢«æ¸…é™¤
            await asyncio.sleep(2)
            
            # éªŒè¯å½“å‰ URLï¼ˆè°ƒè¯•ç”¨ï¼‰
            current_url = page.url
            logging.info(f"Current URL after navigation: {current_url}")
            
            # è¿”å› Feed é¡µé¢
            try:
                logging.info("Navigating back to Feed page")
                await page.goto("https://www.linkedin.com/feed/", timeout=60000)
                await page.wait_for_load_state('domcontentloaded', timeout=10000)
                logging.info("âœ… Navigated back to Feed page after clearing notification")
            except Exception as nav_err:
                logging.error(f"âŒ Failed to navigate back to Feed page: {nav_err}")

        except Exception as e:
            logging.error(
                f"âŒ Error clearing {notification_source} notification: {e}",
                exc_info=True
            )

    async def _notify_business_conversations(self, data: List[dict], source: str):
        """é€šçŸ¥ä½¿ç”¨æ–¹ï¼šå¯¹è¯æ›´æ–°æ•°æ®

        Args:
            data: æ›´æ–°çš„å¯¹è¯/å¥½å‹æ•°æ®åˆ—è¡¨
            source: æ•°æ®æºç±»å‹ ('message' æˆ–å…¶ä»–)
        """
        if not data:
            return False

        # ä»é…ç½®ä¸­è·å– Callback æ¥å£ URLï¼ˆä½¿ç”¨ sync_to_async é¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
        try:
            account_model = await MonitorAccount.objects.aget(id=int(self.account_id))
            callback_url = account_model.callback_url
            callback_token = account_model.callback_token
            hash_id = account_model.hash_id
            
            # æ„å»ºè¯·æ±‚å¤´
            callback_headers = {
                "Content-Type": "application/json"
            }
            if callback_token:
                callback_headers["X-Callback-Token"] = callback_token
        except MonitorAccount.DoesNotExist:
            logging.error(f"è´¦å·IDï¼š{self.account_id} ä¸å­˜åœ¨")
            return False
        
        # å‡†å¤‡é€šçŸ¥æ•°æ®ï¼ˆéœ€è¦åºåˆ—åŒ– datetime å¯¹è±¡ï¼‰
        def serialize_value(value):
            """é€’å½’åºåˆ—åŒ–å€¼ï¼Œå°† datetime å¯¹è±¡è½¬æ¢ä¸º ISO æ ¼å¼å­—ç¬¦ä¸²"""
            if isinstance(value, datetime):
                return value.isoformat()
            elif isinstance(value, dict):
                return {k: serialize_value(v) for k, v in value.items()}
            elif isinstance(value, (list, tuple)):
                return [serialize_value(item) for item in value]
            else:
                return value
        
        def serialize_data(data_list):
            """åºåˆ—åŒ–æ•°æ®åˆ—è¡¨ï¼Œå¤„ç†æ‰€æœ‰å±‚çº§çš„ datetime å¯¹è±¡"""
            return [serialize_value(item) for item in data_list]
        
        serialized_data = serialize_data(data)
        
        if source == 'message':
            json_data = {'conversations': serialized_data, 'profile_id': hash_id, 'type':'conversations'}
            notify_type = 'æ¶ˆæ¯åˆ—è¡¨'
        else:
            json_data = {'connections': serialized_data, 'profile_id': hash_id, 'type':'connections'}
            notify_type = 'å¥½å‹åˆ—è¡¨'
        
        # æ•°æ®é‡æ£€æŸ¥ï¼ˆå¦‚æœæ•°æ®é‡å¾ˆå¤§ï¼Œè®°å½•è­¦å‘Šï¼‰
        if len(data) > 100:
            logging.warning(
                f"å‡†å¤‡å‘é€å¤§é‡æ•°æ®åˆ° Callback URLï¼Œ"
                f"æ•°æ®é‡ï¼š{len(data)} æ¡ï¼Œå¯èƒ½å¯¼è‡´è¯·æ±‚è¶…æ—¶æˆ–å¤±è´¥"
            )
        
        # å¦‚æœé…ç½®äº†å›è°ƒ URLï¼Œå°è¯•é€šçŸ¥
        if callback_url:
            # è®°å½•è¯·æ±‚ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            logging.info(
                f"å‡†å¤‡é€šçŸ¥ Business æ–¹{notify_type}æ•°æ®ï¼Œ"
                f"URL: {callback_url}ï¼Œ"
                f"æ•°æ®é‡: {len(data)} æ¡ï¼Œ"
                f"æ˜¯å¦åŒ…å« Token: {bool(callback_token)}"
            )
            
            for retry in range(5):
                try:
                    # ä½¿ç”¨å¼‚æ­¥ HTTP è¯·æ±‚ï¼ˆé¿å…é˜»å¡äº‹ä»¶å¾ªç¯ï¼‰
                    loop = asyncio.get_running_loop()
                    
                    # åœ¨ executor ä¸­æ‰§è¡Œè¯·æ±‚ï¼ˆåŒ…å« JSON åºåˆ—åŒ–ï¼‰
                    def make_request():
                        try:
                            return requests.post(
                                callback_url, 
                                json=json_data, 
                                headers=callback_headers,
                                timeout=10
                            )
                        except (TypeError, ValueError) as json_err:
                            # æ•è· JSON åºåˆ—åŒ–é”™è¯¯
                            logging.error(
                                f"JSON åºåˆ—åŒ–å¤±è´¥: {json_err}ï¼Œ"
                                f"æ•°æ®ç±»å‹æ£€æŸ¥ï¼š{type(json_data)}",
                                exc_info=True
                            )
                            raise
                    
                    response = await loop.run_in_executor(None, make_request)
                    
                    if 200 <= response.status_code < 300:
                        logging.info(
                            f"é€šçŸ¥ Business æ–¹{notify_type}æ•°æ®æ›´æ–°æˆåŠŸï¼Œ"
                            f"è´¦å·IDï¼š{self.account_id}ï¼Œæ›´æ–°æ•°é‡ï¼š{len(data)}"
                        )
                        return True
                    else:
                        # å°è¯•è·å–å“åº”å†…å®¹ç”¨äºè°ƒè¯•
                        try:
                            response_text = response.text[:500]  # åªè®°å½•å‰500å­—ç¬¦
                        except Exception as text_err:
                            response_text = f"æ— æ³•è·å–å“åº”å†…å®¹: {text_err}"
                        
                        logging.warning(
                            f"é€šçŸ¥ Business æ–¹å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}ï¼Œ"
                            f"å“åº”å†…å®¹ï¼š{response_text}ï¼Œ"
                            f"é‡è¯•æ¬¡æ•°ï¼š{retry + 1}/5"
                        )
                except requests.exceptions.Timeout as e:
                    logging.error(
                        f"é€šçŸ¥ Business æ–¹è¶…æ—¶ï¼š{e}ï¼Œé‡è¯•æ¬¡æ•°ï¼š{retry + 1}/5",
                        exc_info=True
                    )
                except requests.exceptions.ConnectionError as e:
                    logging.error(
                        f"é€šçŸ¥ Business æ–¹è¿æ¥é”™è¯¯ï¼š{e}ï¼Œé‡è¯•æ¬¡æ•°ï¼š{retry + 1}/5",
                        exc_info=True
                    )
                except requests.exceptions.RequestException as e:
                    logging.error(
                        f"é€šçŸ¥ Business æ–¹è¯·æ±‚å¼‚å¸¸ï¼š{e}ï¼Œé‡è¯•æ¬¡æ•°ï¼š{retry + 1}/5",
                        exc_info=True
                    )
                except Exception as e:
                    logging.error(
                        f"é€šçŸ¥ Business æ–¹æœªçŸ¥å¼‚å¸¸ï¼š{e}ï¼Œé‡è¯•æ¬¡æ•°ï¼š{retry + 1}/5",
                        exc_info=True
                    )
                
                # å¦‚æœä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•ï¼Œç­‰å¾…åé‡è¯•
                if retry < 4:
                    await asyncio.sleep(5)  # ä½¿ç”¨å¼‚æ­¥ sleep
            
            # æ‰€æœ‰é‡è¯•å¤±è´¥ï¼Œå‘é€å‘Šè­¦
            logging.error(
                f"é€šçŸ¥ Business æ–¹{notify_type}æ•°æ®æ›´æ–°å¤±è´¥ï¼ˆ5æ¬¡é‡è¯•å‡å¤±è´¥ï¼‰ï¼Œ"
                f"è´¦å·IDï¼š{self.account_id}ï¼Œæ›´æ–°æ•°é‡ï¼š{len(data)}"
            )
            await sync_to_async(send_wechat_message)(
                f'æ•°æ®ç›‘æ§-{notify_type}æ•°æ®æ›´æ–°é€šçŸ¥å¤±è´¥ï¼Œ'
                f'è´¦å·IDï¼š{self.account_id}ï¼Œæ›´æ–°æ•°é‡ï¼š{len(data)}',
                key=WechatRobotKey.TEST_WECHAT_ROBOT_KEY.value
            )
            return False
        else:
            # æœªé…ç½®å›è°ƒ URLï¼Œå‘é€å¾®ä¿¡é€šçŸ¥
            logging.warning(f"è´¦å·IDï¼š{self.account_id} æœªé…ç½® callback_url")
            await sync_to_async(send_wechat_message)(
                f'æ•°æ®ç›‘æ§-{notify_type}æ•°æ®æ›´æ–°é€šçŸ¥ï¼ˆæœªé…ç½®å›è°ƒURLï¼‰ï¼Œ'
                f'è´¦å·IDï¼š{self.account_id}ï¼Œæ›´æ–°æ•°é‡ï¼š{len(data)}',
                key=WechatRobotKey.TEST_WECHAT_ROBOT_KEY.value
            )
            return False
